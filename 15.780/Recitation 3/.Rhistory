}
attempt <- try(test)
attempt
attempt <- try( z<- 2+'b', silent=TRUE)
attempt
if (class(attempt) == "try-error"){
z <- 3
}
z
try(z<-2+3, silent=TRUE)
z
v <- try(z<-2+3, silent=TRUE)
class(v)
library(hash)
library(dict)
install.packages('hash')
proper_names <- c("vol_boxes_wkly",
"fcast_vol_boxes_wkly",
"ft_headcount_wkly",
"pt_headcount_wkly",
"temp_headcount_wkly",
"tot_headcount_wkly",
"tot_sales",
"tot_sales",
#"var_tot_sales",
"tot_damages",
"damages_per_sales",
"danages_per_sales",
"fcast_damages_per_sales",
"fcast_damages_per_sales",
#"var_damages",
"box_per_hr",
#"var_prod",
"labor$_per_box",
"tot_prod_hrs",
"part_time_per_tot_hrs",
"tot_hrs",
"receiving_cartons",
"fcast_receiving_cartons",
"fcast_receiving_cartons",
#"var_receiving",
"missing_wrong",
"missing_wrong",
#"var_missing_wrong",
"cargo_loss",
"fcast_cargo_loss",
#"var_cargo_loss",
"leave_behinds_per_tot",
"labor_per_sales",
"labor_per_sales",
"labor_per_sales",
"fcast_labor_per_sales",
#"var_labor_rate",
"prod_avg_wage",
"fcast_prod_avg_wage")
#"var_prod_avg_wage")
keep_cols <- hash("vol_boxes_wkly" = c("Boxes_Boxes - Actual"),
"fcast_vol_boxes_wkly" = c("Boxes_Boxes - Budget"),
"ft_headcount_wkly" = c("Headcount_Non-Exempt (F)"),
"pt_headcount_wkly" = c("Headcount_Non-Exempt (P)"),
"temp_headcount_wkly" = c("Headcount_Temp"),
"tot_headcount_wkly" = c("Headcount_Productive"),
"tot_sales" = c("Total Sales Serviced (Incl Wrap & Label)",
"Total Net Sales"),
# older files have Total Net Sales, newer files have Total Sales Serviced (Incl Wrap & Label)
"tot_damages" = c("Damages_Total Damages $"), # check 2020 - has Damages %_Total Damages
"damages_per_sales" = c("Damages_Damages as a % of Sales - Actual",
"Damages_Damages as a % of Sales Serv - Actual"),
"fcast_damages_per_sales" = c("Damages_Damages as a % of Sales - Budget",
"Damages_Damages as a % of Sales - LY"),
"box_per_hr" = c("Productivity_Box Per Productive Hour"),
"labor_per_box" = c("Productivity_Labor $ Per Box"),
"tot_prod_hrs" = c("Hours_Total Productive Hours"),
"part_time_per_tot_hrs" = c("Hours_Part Time as a % of Total Hours"),
"ot_hrs" = c("Hours_OT Hours"),
"receiving_cartons" = c("Receiving_Receiving Cartons - Actual"),
"fcast_receiving_cartons" = c("Receiving_Receiving Cartons - Budget",
"Receiving_Receiving Cartons - Budget (Spls FC Only)"),
"missing_wrong" = c("Missing & Wrong_Combined Contract & .Com",
"Missing & Wrong_Combined Contract & SBD "),
"cargo_loss" = c("Cargo Loss_Actual"),
"fcast_cargo_loss" = c("Cargo Loss_Budget"),
"leave_behinds_per_tot" = c("Leave Behinds_% of Total Cartons"),
"labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Actual"),
"fcast_labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Budget",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Budget (Incl Challenge)",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Adj .Budget"),
"prod_avg_wage" = c("Average Wage - Productive_Average Wage - Actual"),
"fcast_prod_avg_wage" = c("Average Wage - Productive_Average Wage - Budget"))
h <- hash('a' = c(1,2,3))
library(hash)
keep_cols <- hash("vol_boxes_wkly" = c("Boxes_Boxes - Actual"),
"fcast_vol_boxes_wkly" = c("Boxes_Boxes - Budget"),
"ft_headcount_wkly" = c("Headcount_Non-Exempt (F)"),
"pt_headcount_wkly" = c("Headcount_Non-Exempt (P)"),
"temp_headcount_wkly" = c("Headcount_Temp"),
"tot_headcount_wkly" = c("Headcount_Productive"),
"tot_sales" = c("Total Sales Serviced (Incl Wrap & Label)",
"Total Net Sales"),
# older files have Total Net Sales, newer files have Total Sales Serviced (Incl Wrap & Label)
"tot_damages" = c("Damages_Total Damages $"), # check 2020 - has Damages %_Total Damages
"damages_per_sales" = c("Damages_Damages as a % of Sales - Actual",
"Damages_Damages as a % of Sales Serv - Actual"),
"fcast_damages_per_sales" = c("Damages_Damages as a % of Sales - Budget",
"Damages_Damages as a % of Sales - LY"),
"box_per_hr" = c("Productivity_Box Per Productive Hour"),
"labor_per_box" = c("Productivity_Labor $ Per Box"),
"tot_prod_hrs" = c("Hours_Total Productive Hours"),
"part_time_per_tot_hrs" = c("Hours_Part Time as a % of Total Hours"),
"ot_hrs" = c("Hours_OT Hours"),
"receiving_cartons" = c("Receiving_Receiving Cartons - Actual"),
"fcast_receiving_cartons" = c("Receiving_Receiving Cartons - Budget",
"Receiving_Receiving Cartons - Budget (Spls FC Only)"),
"missing_wrong" = c("Missing & Wrong_Combined Contract & .Com",
"Missing & Wrong_Combined Contract & SBD "),
"cargo_loss" = c("Cargo Loss_Actual"),
"fcast_cargo_loss" = c("Cargo Loss_Budget"),
"leave_behinds_per_tot" = c("Leave Behinds_% of Total Cartons"),
"labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Actual"),
"fcast_labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Budget",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Budget (Incl Challenge)",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Adj .Budget"),
"prod_avg_wage" = c("Average Wage - Productive_Average Wage - Actual"),
"fcast_prod_avg_wage" = c("Average Wage - Productive_Average Wage - Budget"))
keep_cols['vol_boxes_wkly']
keep_cols[['vol_boxes_wkly']]
keep_cols[['vol_boxes_wkly']][1]
keys(keep_cols)
abs(-65)
library(ggfortify)
## Loading required package: ggplot2
library(tseries)
library(forecast)
install.packages(c('ggfortify', 'tseries', 'forecast'))
library(ggfortify)
## Loading required package: ggplot2
library(tseries)
library(forecast)
data(AirPassengers)
AP <- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)
# PROBLEM 1: MISSING VALUES
data(Glass , package = "mlbench")
uncor = Glass
# 1) introduce 30 missing values each in the Si and K variables
set.seed(80)
Glass$Si[sample(1:nrow(Glass), 30)] = NA
Glass$K[sample(1:nrow(Glass), 30)] = NA
impute(Glass$Si, median)
# function to calculate MAPE
MAPE = function(actual, imputed) {
percent_errors = abs(actual - imputed) / abs(actual)
return(mean(percent_errors))
}
# function to calculate MAE
MAE = function(actual, imputed) {
errors = abs(actual - imputed)
return(mean(errors))
}
sq_errors = (actual - imputed)^2
#' compute MAPE of our imputation
org   = uncor$Si               # uncorrupted data
imp   = impute(Glass$Si, median)  # My imputation
miss  = is.na(Glass$Si)         # T/F which elements were NA
MAPE(org[miss], imp[miss])   # 0.009065373
library(mlbench)
library(Hmisc)
library(DMwR)
library(editrules)
imp   = impute(Glass$Si, median)  # My imputation
MAPE(org[miss], imp[miss])   # 0.009065373
MAE(org[miss], imp[miss]) # 0.650087
MSE(org[miss], imp[miss])  #0.7439585
return(mean(sq_errors))
sq_errors = (actual - imputed)^2
MSE = function(actual, imputed) {
sq_errors = (actual - imputed)^2
return(mean(sq_errors))
}
MSE(org[miss], imp[miss])  #0.7439585
MAPE(org[miss], imp[miss])   # 0.009065373
MAE(org[miss], imp[miss]) # 0.650087
MSE(org[miss], imp[miss])  #0.7439585
library(mlbench)
library(Hmisc)
library(DMwR)
library(editrules)
# PROBLEM 1: MISSING VALUES
data(Glass , package = "mlbench")
# keep a copy of the original data
original = Glass
# 1) introduce 30 missing values each in the Si and K variables
set.seed(80)
Glass$Si[sample(1:nrow(Glass), 30)] = NA
Glass$K[sample(1:nrow(Glass), 30)] = NA
# 3) Impute the variable Si with median
#    report the MAE, MSE and MAPE values to evaluate the accuracy of this imputation
imp = impute(Glass$Si, median)
org = original$Si
# find the cells where we introduced NAs
miss  = is.na(Glass$Si)
MAE(org[miss], imp[miss])
MSE(org[miss], imp[miss])
MAPE(org[miss], imp[miss])
library(mice)
# PROBLEM 1: MISSING VALUES
data(Glass , package = "mlbench")
# keep a copy of the original data
original = Glass
# 1) introduce 30 missing values each in the Si and K variables and graph pattern
set.seed(80)
Glass$Si[sample(1:nrow(Glass), 30)] = NA
Glass$K[sample(1:nrow(Glass), 30)] = NA
md.pattern(Glass)
# 3) Impute the variable Si with median
#    report the MAE, MSE and MAPE values to evaluate the accuracy of this imputation
imp = impute(Glass$Si, median)
org = original$Si
# find the cells where we introduced NAs
miss  = is.na(Glass$Si)
MAE(org[miss], imp[miss])
MSE(org[miss], imp[miss])
MAPE(org[miss], imp[miss])
view(Glass)
View(Glass)
View(Glass)
#Loading in the Glass dataset from mlbench
install.packages("Rtools")
install.packages("mlbench")
data(Glass, package = "mlbench")
#set the seed to generate 30 missing values
#sample gets random numbers from a sequence
original = Glass
set.seed(80)
Glass$Si[sample(1:nrow(Glass), 30)]= NA
Glass$K[sample(1:nrow(Glass), 30)] = NA
#use the md.pattern function from mice library to show pattern of missing values
library(mice)
md.pattern(Glass)
#use impute() to impute the variable Si with it's mean value
library(Hmisc)
impute(Glass$Si, mean)
#impute the variable Si with median value and report the MAE, MSE, and MAPE values to evalute accuracy
data(Glass, package = "mlbench")
set.seed(80)
Glass$Si[sample(1:nrow(Glass), 30)]= NA
Glass$K[sample(1:nrow(Glass), 30)] = NA
#function to calculate the MAE
MAE = function(actual, imputed)
{errors=abs(actual-imputed)
return(mean(errors))
}
#function to calculate the MSE
MSE = function(actual, imputed) {
squared_errors = (actual-imputed)^2
return(mean(squared_errors))
}
#function to calculate the MAPE
MAPE = function(actual, imputed) {
percent_errors = abs(actual - imputed) / abs(actual)
return(mean(percent_errors))
}
#Let's use the function to compute MAE, MSE, and MAPE of our imputation
org   = original$Si            # The uncorrupted data
imp   = impute(Glass$Si,median) #median imputation
miss  = is.na(Glass$Si) #T/F which elements were NA
MAE(org[miss], imp[miss])
MSE(org[miss], imp[miss])
MAPE(org[miss], imp[miss])
install.packages("mlbench")
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
str(iris)
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
hist(iris$Sepal.width, breaks=30, xlab = "Sepal Width")
# 3) create a simple ruleset that contains the rules that the columns of the dataframe should obey
# Sepal.width should not have values over 10, Petal.Length cannot be negative:
# editset tells which rows follow the rules defined
E = editset(c("Sepal.Width < 10", "Petal.Length > 0"))
# violatededits gives which rows violated the rules (True if violated)
V = violatedEdits(E, iris)
# 4) number of observations that violate each of the rules
# V[, 'num1'] gives which rows violated Sepal.Width < 10
sum(V[,"num1"])
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
setwd("~/Dropbox (MIT)/Senior/15.780/hw1")
# 1) Load iris.csv, use the str() function to get a first look at the data
#    check if any columns need to be coerced into a different data type
iris = read.csv('iris.csv')
# 2) Make a histogram for each numeric column
hist(iris$Sepal.Length, breaks=30, xlab = "Sepal Length")
hist(iris$Sepal.Width, breaks=30, xlab = "Sepal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
hist(iris$Petal.Length, breaks=30, xlab = "Petal Length")
hist(iris$Petal.Width, breaks=30, xlab = "Petal Width")
# 3) create a simple ruleset that contains the rules that the columns of the dataframe should obey
# Sepal.width should not have values over 10, Petal.Length cannot be negative:
# editset tells which rows follow the rules defined
E = editset(c("Sepal.Width < 10", "Petal.Length > 0"))
# violatededits gives which rows violated the rules (True if violated)
V = violatedEdits(E, iris)
# 4) number of observations that violate each of the rules
# V[, 'num1'] gives which rows violated Sepal.Width < 10
sum(V[,"num1"])
#  V[, 'num2'] gives which rows violated Petal.Length > 0
sum(V[, 'num2'])
# 5) Filter iris to contain only observations that do not violate any of the rules
iris_filtered = na.omit(subset(iris, !V[, 'num1'] & !V[,'num2']))
dim(iris_filtered)
#' -----------------------------------------------------------------
#' REGRESSION
#' -----------------------------------------------------------------
#' Let's load the `abalone` dataset from last time and do some
#' regression! If you haven't already, you will need to install
#' the `PivotalR` package:
#' install.packages("PivotalR")
data(abalone, package="PivotalR")
#' Our first step is always to understand our data (see last Recitation).
#' I'll just remove some unnecessary columns to make things easier...
abalone = abalone[,c("shucked","whole", "length", "height", "diameter")]
#' Next we always want to split our data into a *training* and *test*
#' set. There's many ways (and packages to do this) but let's do it
#' randomly. We use `sample()` to pick out ~70% of rows (or rather
#' their indices) to be in the training set:
set.seed(15)
idx_train = sample(1:nrow(abalone), round(0.7 * nrow(abalone)))
idx_train
train = abalone[idx_train, ]
test = abalone[-idx_train, ] # keeps all indices that are not in idx_train
#' The `cor()` function can help us figure out correlations (and examine
#' colinearity) in our training set:
cor(train$whole, train$length)
#' Or more importantly, do a pairwise correlation matrix:
cor(train)
cor(train[,-1]) #' Except for the first column (dependent variable)
View(train)
View(train)
cor(train[,2:])
cor(train[,2:ncol(train)])
#' Nearly all of our features are highly correlated, so let's build
#' just a simple model predicting `shucked` weight from `whole` weight:
mod = lm(shucked ~ whole, data = train)
#' Nearly all of our features are highly correlated, so let's build
#' just a simple model predicting `shucked` weight from `whole` weight:
#' formula: y ~ x, data to train on
#' don't need dollar signs bc pass in dataframe as an arg
mod = lm(shucked ~ whole, data = train)
mod
#' We can look at the model using `summary()`:
summary(mod)
#' The `~` notation is known in R as a formula, and is very versatile.
#' We can build models with multiple variables:
lm(shucked ~ whole + diameter + length, data = train)
#' Or even use the shorthand `.` to use all variables:
lm(shucked ~ ., data = train)
#' We can also extract elements of the model using the *$* notation:
mod$coefficients
mod$fitted.values #' Predicted values in the training set
mod$residuals     #' Prediction errors in the training set
#' We always want to check linearity of our model (is the relationship
#' truly linear)? We can look at the `Fitted Values vs Residuals` plot
#' (see Lecture 2) .
plot(mod$fitted.values, mod$residuals)
abline(h = 0, col = "red")
#' The `predict` function makes... drum roll... predictions on a new df:
predict(mod, newdata = train)
predict(mod, newdata = train) - mod$fitted.values # basically the same
#' More importantly we want to make predictions on the test set:
test_pred = predict(mod, newdata = test)
#' Let's calculate the MAPE for this prediction:
mean( abs(test$shucked - test_pred)  /  abs(test$shucked) )
setwd("~/Dropbox (MIT)/Senior/15.780/Recitation 3")
df = read.csv("Alaska_AvgTemp.csv")
View(df)
class(df$Date)
install.packages("anytime")
#' Working with dates is easy in R with the `anytime` package.
#' install.packages("anytime")
library(anytime)
df$Date = anytime(df$Date)
class(df$Date) # POSIXct is a special data type for time/dates
df$Date
#' Having a specialized data type, makes plotting labels nice:
plot(df$Date, df$AverageTemperature,
type = "l",  # Use lines instead of scatter plot
xlab = "Date", ylab = "Average Temperature")
#' Whenever we make time series models, we want to use the
#' special time-series data type `ts`. The reason is that,
#' beyond the values, we also tell R what seasonal  *frequency*
#' to use, so that models know every how many periods patterns
#' might repeat. So if we have monthly data we would specify
#' *frequency = 12*:
temp = ts(df$AverageTemperature, frequency = 12)
#' `temp` now acts sort of like a vector, but with nicer formatting:
temp
ts(df$AverageTemperature, frequency = 6)
length(temp)
#' The `decompose()` function can do a simple decomposition
#' into trend, seasonal and residual components (see Lecture 5):
decomp = decompose(temp, type = "additive")
#' The `decompose()` function can do a simple decomposition
#' into trend, seasonal and residual components (see Lecture 5):
decomp = decompose(temp)
#' Whenever we make time series models, we want to use the
#' special time-series data type `ts`. The reason is that,
#' beyond the values, we also tell R what seasonal  *frequency*
#' to use, so that models know every how many periods patterns
#' might repeat. So if we have monthly data we would specify
#' you need to tell the model how often you expect seasonal patterns to occur
#' model will not figure this out itself
#' *frequency = 12*:
temp = ts(df$AverageTemperature, frequency = 12)
#' The `decompose()` function can do a simple decomposition
#' into trend, seasonal and residual components (see Lecture 5):
decomp = decompose(temp)
#' The `decompose()` function can do a simple decomposition
#' into trend, seasonal and residual components (see Lecture 5):
decomp = decompose(temp)
sessionInfo()
df = read.csv("Alaska_AvgTemp.csv")
df$Date = anytime(df$Date)
#' Whenever we make time series models, we want to use the
#' special time-series data type `ts`. The reason is that,
#' beyond the values, we also tell R what seasonal  *frequency*
#' to use, so that models know every how many periods patterns
#' might repeat. So if we have monthly data we would specify
#' you need to tell the model how often you expect seasonal patterns to occur
#' model will not figure this out itself
#' *frequency = 12*:
temp = ts(df$AverageTemperature, frequency = 12)
#' The `decompose()` function can do a simple decomposition
#' into trend, seasonal and residual components (see Lecture 5):
decomp = decompose(temp)
decomp$trend    #' Smoothed moving average
decomp$seasonal #' Seasonal amount below or above average
decomp$random   #' The rest
#' With nice plotting functionality:
plot(decomp)
#' Let's load the *forecast* library and run some models:
#' install.packages("forecast")
library(forecast)
#' The `arima()` function let's use fit an ARIMA model with
#' *order = (p, d, q)*:
mod1 = arima(temp, order = c(2, 1, 1))
mod1
#' The `summary()` gives us more detailed information, including
#' a variety of training accuracy metrics:
summary(mod1)
#' You can also get this with `accuracy()`:
accuracy(mod1)
#' We can use the `forecast()` to predict the next *h* values
#' of the time series (including confidence interval):
forecast(mod1, h = 4)
#' The `auto.arima()` does automatic model selection for us, and
#' computes the "best-fit" (p, d, q)(P, D, Q)
?auto.arima
mod2 = auto.arima(temp) # This takes ~30secs
#' We can see what the best-fit parameters were:
arimaorder(mod2)
#' We can also read them (and training accuracy metrics)
#' from the `summary()`:
summary(mod2)
#' As with regression, we can extract the fitted values and
#' residuals:
mod2$fitted
mod2$residuals
#' Keep in mind that these are stored as `ts` objects. You can
#' convert them back with `as.vector()`:
train_pred = as.vector(mod2$fitted)
class(train_pred)
#' And now we can calculate MAPE by ourselves:
mean(abs(train_pred - df$AverageTemperature) / abs(df$AverageTemperature))

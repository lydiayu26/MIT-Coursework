full_dat <- data.frame(date=double(),
FC_ID=character(),
FC_name=character(),
department=character(),
department_num=character(),
reg_hours=double(),
ot_hours=double(),
temp_hours=double(),
temp_ot=double(),
percent_ot=character(),
total_hours=double(),
budget_hours=double(),
diff_total_budget_hours=double(),
units=double(),
productivity=double(),
standard=double(),
volume_hours_percent=character(),
efficiency=character(),
stringsAsFactors=FALSE)
# we only care about the sheets with daily info
sheets <-  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
for (i in 1:length(unprocessed_files[1])){
# process each of the M-F sheets
for (sheet in sheets){
skip_iter <- FALSE
tryCatch(new_dat  <- read_data(file.path(directory, unprocessed_files[i]), sheet),
error = function(e) {
message("Unable to read sheet ", sheet, " from file ", file_list[i], "\n", e)
skip_iter <- TRUE
})
if(skip_iter) next
full_dat <- rbind(full_dat, new_dat)
}
}
# format filedate as a Date
full_dat$date <- as.Date(full_dat$date, origin="1899-12-30")
# adjust order of columns to have identifiers at beginning
full_dat <- full_dat %>%
select(c(date, FC_ID, FC_name), everything())
# save full_dat as csv
options(scipen=999)
##################################################
## Project: Aggregate DOR files
## Script purpose: This script aggregates labor_productivity spreadsheets into a single table
## Date created: 03/29/21
## Author: Lydia Yu
##################################################
library(tidyverse)
library(readxl)
library(data.table)
options(scipen = 999)
# function that returns a list of files that haven't been aggregated yet
get_unprocessed_files <- function(file_list){
# get the existing aggregated DOR data and use it to determine which files in the directory
# haven't been processed yet. If the aggregate file doesn't exist, just take all the DOR files.
existing_csv_path <-"/Users/lydiayu/Dropbox (MIT)/urop_lydia/data/admin_data/clean/dor/dor_aggregate.csv"
if(file.exists(existing_csv_path)){
existing_csv <- read_csv(existing_csv_path)
# find which files have already been aggregated
processed_files <- existing_csv[["filename"]]
# get just the base filenames for everything in file_list
# file_list <- lapply(file_list, function(x) basename(x))
# get the elements from file_list that aren't in processed_files
unprocessed_files <- setdiff(file_list, processed_files)
} else {
unprocessed_files = file_list
}
return(unprocessed_files)
}
# Function to pull the data off the sheets
read_data <- function(file, sheet){
dat <- suppressMessages(read_excel(path=file, sheet=sheet, col_names=FALSE))
dat <- as.data.frame(dat)
# Delete rows that are totally blank
dat <- filter_all(dat, any_vars(!is.na(.)))
# find date (always located in 4th row, 1st col after filtering for NAs)
date <- as.numeric(dat[4, 1])
# find FC ID/Name (always located in 2nd row, 1st col after filtering for NAs)
fc_name_id <- dat[2, 1]
# get components from fc_name_id; 1st group is string that contains pattern, 2nd group is any chars, 3rd is digits (ID), 4th is any chars (in case fc_name came after fc_id)
fc_name_id_components <- str_match(fc_name_id, "(?:(.*) )?(\\d+)(?: (.*))?")
if(!all(is.na(fc_name_id_components))){
# there are non-NA values; assign components accordingly
fc_name <- if(!is.na(fc_name_id_components[2])) fc_name_id_components[2] else fc_name_id_components[4]
fc_id <- fc_name_id_components[3]
} else {
# we couldn't find any digits, so everything element in fc_name_id_components is NA
# fill name in with result from fc_name_id because we know that must contain a name and not an ID number
fc_name <- fc_name_id
}
# find the row index that "TOTAL HOURS" is located at, keep everything before that
# check each cell to see if it contains "TOTAL HOURS"; return matrix of T/F for each cell
contains_totalhours <- apply(dat, c(1,2), (function(cell) grepl("TOTAL HOURS", cell)))
# find the idx of the row that contains "TOTAL HOURS"
totalhours_row_ind = which(contains_totalhours, arr.ind=TRUE)[1]
# keep just the rows that come before "TOTAL HOURS"
# data starts at the 6th row, with 5th row being col names
dat <- dat[6:totalhours_row_ind-1, ]
# set first row to colnames
names(dat) <- dat[1,]
dat <- dat[-1,]
# rename columns
dat <- dat %>%
rename("department" = "Department",
"department_num" = "Dept.#",
"reg_hours" = "Reg. Hours",
"ot_hours"="OT Hours",
"temp_hours" = "Temp Hours",
"temp_ot" = "Temp OT",
"percent_ot" = "% of Ot",
"total_hours" = "Total Hours",
"budget_hours" = "Budget Hours",
"diff_total_budget_hours" = "(Total Hrs - Budget Hrs)",
"units" = "Cartons or Unit of Mea.",
"productivity" = "Productivity (/Hr.)",
"standard" = "Standard (Hr.)",
"volume_hours_percent" = "Volume & Hours %",
"efficiency" = "Efficiency")
# remove rows where department is blank (these rows are all NA except for volume_hours_percent)
dat <- dat[!is.na(dat$department), ]
# add columns tracking date, FC ID, and FC Name
dat$date <- date
dat$FC_ID <- fc_id
dat$FC_name <- fc_name
return(dat)
}
# get the files that we want to aggregate
directory <- "/Users/lydiayu/Dropbox (MIT)/urop_lydia/data/admin_data/source/dor"
file_list <- dir(path=directory, full.names=FALSE)  # list of all files under this directory
# take just the ones that haven't been aggregated already (this list contains just basenames of the files)
unprocessed_files <- get_unprocessed_files(file_list)
full_dat <- data.frame(date=double(),
FC_ID=character(),
FC_name=character(),
department=character(),
department_num=character(),
reg_hours=double(),
ot_hours=double(),
temp_hours=double(),
temp_ot=double(),
percent_ot=character(),
total_hours=double(),
budget_hours=double(),
diff_total_budget_hours=double(),
units=double(),
productivity=double(),
standard=double(),
volume_hours_percent=character(),
efficiency=character(),
stringsAsFactors=FALSE)
# we only care about the sheets with daily info
sheets <-  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
for (i in 1:length(unprocessed_files[1])){
# process each of the M-F sheets
for (sheet in sheets){
skip_iter <- FALSE
tryCatch(new_dat  <- read_data(file.path(directory, unprocessed_files[i]), sheet),
error = function(e) {
message("Unable to read sheet ", sheet, " from file ", file_list[i], "\n", e)
skip_iter <- TRUE
})
if(skip_iter) next
full_dat <- rbind(full_dat, new_dat)
}
}
# format filedate as a Date
full_dat$date <- as.Date(full_dat$date, origin="1899-12-30")
# adjust order of columns to have identifiers at beginning
full_dat <- full_dat %>%
select(c(date, FC_ID, FC_name), everything())
# save full_dat as csv
(dat, round, 3)
##################################################
## Project: Aggregate DOR files
## Script purpose: This script aggregates labor_productivity spreadsheets into a single table
## Date created: 03/29/21
## Author: Lydia Yu
##################################################
library(tidyverse)
library(readxl)
library(data.table)
options(scipen = 999)
# function that returns a list of files that haven't been aggregated yet
get_unprocessed_files <- function(file_list){
# get the existing aggregated DOR data and use it to determine which files in the directory
# haven't been processed yet. If the aggregate file doesn't exist, just take all the DOR files.
existing_csv_path <-"/Users/lydiayu/Dropbox (MIT)/urop_lydia/data/admin_data/clean/dor/dor_aggregate.csv"
if(file.exists(existing_csv_path)){
existing_csv <- read_csv(existing_csv_path)
# find which files have already been aggregated
processed_files <- existing_csv[["filename"]]
# get just the base filenames for everything in file_list
# file_list <- lapply(file_list, function(x) basename(x))
# get the elements from file_list that aren't in processed_files
unprocessed_files <- setdiff(file_list, processed_files)
} else {
unprocessed_files = file_list
}
return(unprocessed_files)
}
# Function to pull the data off the sheets
read_data <- function(file, sheet){
dat <- suppressMessages(read_excel(path=file, sheet=sheet, col_names=FALSE))
dat <- as.data.frame(dat)
# Delete rows that are totally blank
dat <- filter_all(dat, any_vars(!is.na(.)))
# find date (always located in 4th row, 1st col after filtering for NAs)
date <- as.numeric(dat[4, 1])
# find FC ID/Name (always located in 2nd row, 1st col after filtering for NAs)
fc_name_id <- dat[2, 1]
# get components from fc_name_id; 1st group is string that contains pattern, 2nd group is any chars, 3rd is digits (ID), 4th is any chars (in case fc_name came after fc_id)
fc_name_id_components <- str_match(fc_name_id, "(?:(.*) )?(\\d+)(?: (.*))?")
if(!all(is.na(fc_name_id_components))){
# there are non-NA values; assign components accordingly
fc_name <- if(!is.na(fc_name_id_components[2])) fc_name_id_components[2] else fc_name_id_components[4]
fc_id <- fc_name_id_components[3]
} else {
# we couldn't find any digits, so everything element in fc_name_id_components is NA
# fill name in with result from fc_name_id because we know that must contain a name and not an ID number
fc_name <- fc_name_id
}
# find the row index that "TOTAL HOURS" is located at, keep everything before that
# check each cell to see if it contains "TOTAL HOURS"; return matrix of T/F for each cell
contains_totalhours <- apply(dat, c(1,2), (function(cell) grepl("TOTAL HOURS", cell)))
# find the idx of the row that contains "TOTAL HOURS"
totalhours_row_ind = which(contains_totalhours, arr.ind=TRUE)[1]
# keep just the rows that come before "TOTAL HOURS"
# data starts at the 6th row, with 5th row being col names
dat <- dat[6:totalhours_row_ind-1, ]
# set first row to colnames
names(dat) <- dat[1,]
dat <- dat[-1,]
# rename columns
dat <- dat %>%
rename("department" = "Department",
"department_num" = "Dept.#",
"reg_hours" = "Reg. Hours",
"ot_hours"="OT Hours",
"temp_hours" = "Temp Hours",
"temp_ot" = "Temp OT",
"percent_ot" = "% of Ot",
"total_hours" = "Total Hours",
"budget_hours" = "Budget Hours",
"diff_total_budget_hours" = "(Total Hrs - Budget Hrs)",
"units" = "Cartons or Unit of Mea.",
"productivity" = "Productivity (/Hr.)",
"standard" = "Standard (Hr.)",
"volume_hours_percent" = "Volume & Hours %",
"efficiency" = "Efficiency")
# remove rows where department is blank (these rows are all NA except for volume_hours_percent)
dat <- dat[!is.na(dat$department), ]
# add columns tracking date, FC ID, and FC Name
dat$date <- date
dat$FC_ID <- fc_id
dat$FC_name <- fc_name
return(dat)
}
# get the files that we want to aggregate
directory <- "/Users/lydiayu/Dropbox (MIT)/urop_lydia/data/admin_data/source/dor"
file_list <- dir(path=directory, full.names=FALSE)  # list of all files under this directory
# take just the ones that haven't been aggregated already (this list contains just basenames of the files)
unprocessed_files <- get_unprocessed_files(file_list)
full_dat <- data.frame(date=double(),
FC_ID=character(),
FC_name=character(),
department=character(),
department_num=character(),
reg_hours=double(),
ot_hours=double(),
temp_hours=double(),
temp_ot=double(),
percent_ot=character(),
total_hours=double(),
budget_hours=double(),
diff_total_budget_hours=double(),
units=double(),
productivity=double(),
standard=double(),
volume_hours_percent=character(),
efficiency=character(),
stringsAsFactors=FALSE)
# we only care about the sheets with daily info
sheets <-  c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday")
for (i in 1:length(unprocessed_files[1])){
# process each of the M-F sheets
for (sheet in sheets){
skip_iter <- FALSE
tryCatch(new_dat  <- read_data(file.path(directory, unprocessed_files[i]), sheet),
error = function(e) {
message("Unable to read sheet ", sheet, " from file ", file_list[i], "\n", e)
skip_iter <- TRUE
})
if(skip_iter) next
full_dat <- rbind(full_dat, new_dat)
}
}
# format filedate as a Date
full_dat$date <- as.Date(full_dat$date, origin="1899-12-30")
# adjust order of columns to have identifiers at beginning
full_dat <- full_dat %>%
select(c(date, FC_ID, FC_name), everything())
# save full_dat as csv
write.csv(full_dat, '/Users/lydiayu/Downloads/test.csv')
attempt <- try(a <- 2+'v')
attempt <- try((function() a <- 2+'v'))
attempt
attempt <- try((function(x) a <- 2+'v'))
attempt
class(attempt)
test <- function(dat_temp, dat) {
dat_temp <- rbind(dat_temp, dat[which(dat$var_names == col)])
return(dat_temp)
}
test <- function() {
z <- 5 + 'b'
return(z)
}
attempt <- try(function())
attempt <- try(test)
attempt
class(attempt)
test <- function() {
z <- 5 + 'b'
}
attempt <- try(test)
attempt
attempt <- try( z<- 2+'b', silent=TRUE)
attempt
if (class(attempt) == "try-error"){
z <- 3
}
z
try(z<-2+3, silent=TRUE)
z
v <- try(z<-2+3, silent=TRUE)
class(v)
library(hash)
library(dict)
install.packages('hash')
proper_names <- c("vol_boxes_wkly",
"fcast_vol_boxes_wkly",
"ft_headcount_wkly",
"pt_headcount_wkly",
"temp_headcount_wkly",
"tot_headcount_wkly",
"tot_sales",
"tot_sales",
#"var_tot_sales",
"tot_damages",
"damages_per_sales",
"danages_per_sales",
"fcast_damages_per_sales",
"fcast_damages_per_sales",
#"var_damages",
"box_per_hr",
#"var_prod",
"labor$_per_box",
"tot_prod_hrs",
"part_time_per_tot_hrs",
"tot_hrs",
"receiving_cartons",
"fcast_receiving_cartons",
"fcast_receiving_cartons",
#"var_receiving",
"missing_wrong",
"missing_wrong",
#"var_missing_wrong",
"cargo_loss",
"fcast_cargo_loss",
#"var_cargo_loss",
"leave_behinds_per_tot",
"labor_per_sales",
"labor_per_sales",
"labor_per_sales",
"fcast_labor_per_sales",
#"var_labor_rate",
"prod_avg_wage",
"fcast_prod_avg_wage")
#"var_prod_avg_wage")
keep_cols <- hash("vol_boxes_wkly" = c("Boxes_Boxes - Actual"),
"fcast_vol_boxes_wkly" = c("Boxes_Boxes - Budget"),
"ft_headcount_wkly" = c("Headcount_Non-Exempt (F)"),
"pt_headcount_wkly" = c("Headcount_Non-Exempt (P)"),
"temp_headcount_wkly" = c("Headcount_Temp"),
"tot_headcount_wkly" = c("Headcount_Productive"),
"tot_sales" = c("Total Sales Serviced (Incl Wrap & Label)",
"Total Net Sales"),
# older files have Total Net Sales, newer files have Total Sales Serviced (Incl Wrap & Label)
"tot_damages" = c("Damages_Total Damages $"), # check 2020 - has Damages %_Total Damages
"damages_per_sales" = c("Damages_Damages as a % of Sales - Actual",
"Damages_Damages as a % of Sales Serv - Actual"),
"fcast_damages_per_sales" = c("Damages_Damages as a % of Sales - Budget",
"Damages_Damages as a % of Sales - LY"),
"box_per_hr" = c("Productivity_Box Per Productive Hour"),
"labor_per_box" = c("Productivity_Labor $ Per Box"),
"tot_prod_hrs" = c("Hours_Total Productive Hours"),
"part_time_per_tot_hrs" = c("Hours_Part Time as a % of Total Hours"),
"ot_hrs" = c("Hours_OT Hours"),
"receiving_cartons" = c("Receiving_Receiving Cartons - Actual"),
"fcast_receiving_cartons" = c("Receiving_Receiving Cartons - Budget",
"Receiving_Receiving Cartons - Budget (Spls FC Only)"),
"missing_wrong" = c("Missing & Wrong_Combined Contract & .Com",
"Missing & Wrong_Combined Contract & SBD "),
"cargo_loss" = c("Cargo Loss_Actual"),
"fcast_cargo_loss" = c("Cargo Loss_Budget"),
"leave_behinds_per_tot" = c("Leave Behinds_% of Total Cartons"),
"labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Actual"),
"fcast_labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Budget",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Budget (Incl Challenge)",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Adj .Budget"),
"prod_avg_wage" = c("Average Wage - Productive_Average Wage - Actual"),
"fcast_prod_avg_wage" = c("Average Wage - Productive_Average Wage - Budget"))
h <- hash('a' = c(1,2,3))
library(hash)
keep_cols <- hash("vol_boxes_wkly" = c("Boxes_Boxes - Actual"),
"fcast_vol_boxes_wkly" = c("Boxes_Boxes - Budget"),
"ft_headcount_wkly" = c("Headcount_Non-Exempt (F)"),
"pt_headcount_wkly" = c("Headcount_Non-Exempt (P)"),
"temp_headcount_wkly" = c("Headcount_Temp"),
"tot_headcount_wkly" = c("Headcount_Productive"),
"tot_sales" = c("Total Sales Serviced (Incl Wrap & Label)",
"Total Net Sales"),
# older files have Total Net Sales, newer files have Total Sales Serviced (Incl Wrap & Label)
"tot_damages" = c("Damages_Total Damages $"), # check 2020 - has Damages %_Total Damages
"damages_per_sales" = c("Damages_Damages as a % of Sales - Actual",
"Damages_Damages as a % of Sales Serv - Actual"),
"fcast_damages_per_sales" = c("Damages_Damages as a % of Sales - Budget",
"Damages_Damages as a % of Sales - LY"),
"box_per_hr" = c("Productivity_Box Per Productive Hour"),
"labor_per_box" = c("Productivity_Labor $ Per Box"),
"tot_prod_hrs" = c("Hours_Total Productive Hours"),
"part_time_per_tot_hrs" = c("Hours_Part Time as a % of Total Hours"),
"ot_hrs" = c("Hours_OT Hours"),
"receiving_cartons" = c("Receiving_Receiving Cartons - Actual"),
"fcast_receiving_cartons" = c("Receiving_Receiving Cartons - Budget",
"Receiving_Receiving Cartons - Budget (Spls FC Only)"),
"missing_wrong" = c("Missing & Wrong_Combined Contract & .Com",
"Missing & Wrong_Combined Contract & SBD "),
"cargo_loss" = c("Cargo Loss_Actual"),
"fcast_cargo_loss" = c("Cargo Loss_Budget"),
"leave_behinds_per_tot" = c("Leave Behinds_% of Total Cartons"),
"labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Actual"),
"fcast_labor_per_sales" = c("Labor Rate - Exc. Sick & Fixed_Var. Labor % of Net Sales - Budget",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Budget (Incl Challenge)",
"Labor Rate - Exc. Sick & Fixed_Var. Labor % of Sales Serviced - Adj .Budget"),
"prod_avg_wage" = c("Average Wage - Productive_Average Wage - Actual"),
"fcast_prod_avg_wage" = c("Average Wage - Productive_Average Wage - Budget"))
keep_cols['vol_boxes_wkly']
keep_cols[['vol_boxes_wkly']]
keep_cols[['vol_boxes_wkly']][1]
keys(keep_cols)
abs(-65)
library(ggfortify)
## Loading required package: ggplot2
library(tseries)
library(forecast)
install.packages(c('ggfortify', 'tseries', 'forecast'))
library(ggfortify)
## Loading required package: ggplot2
library(tseries)
library(forecast)
data(AirPassengers)
AP <- AirPassengers
# Take a look at the class of the dataset AirPassengers
class(AP)
library(mlogit)
# PROBLEM 1
commute = read.csv('commuteFall2020.csv')
# 1) data preprocessing
# create columns for total travel time -- bus.wait + bus.travel; car.travel
commute$time_CAR = commute$Car.Travel
commute$time_BUS = commute$Bus.Travel + commute$Bus.Wait
# create columns for cost -- BusFare; ParkingCost
commute$cost_CAR = commute$ParkingCost
commute$cost_BUS = commute$BusFare
# drop unnecessary columns from df
commute = subset(commute, select = -c(Car.Travel, Bus.Travel, Bus.Wait, ParkingCost, BusFare))
# data is wide so adding id col is not necessary
# 2) fit MNL model
com = mlogit.data(commute, choice = "Choice", shape = "wide",  varying = 3:6, sep= "_")
head(com)
setwd("~/Dropbox (MIT)/Senior/15.780/hw3")
library(mlogit)
# PROBLEM 1
commute = read.csv('commuteFall2020.csv')
# 1) data preprocessing
# create columns for total travel time -- bus.wait + bus.travel; car.travel
commute$time_CAR = commute$Car.Travel
commute$time_BUS = commute$Bus.Travel + commute$Bus.Wait
# create columns for cost -- BusFare; ParkingCost
commute$cost_CAR = commute$ParkingCost
commute$cost_BUS = commute$BusFare
# drop unnecessary columns from df
commute = subset(commute, select = -c(Car.Travel, Bus.Travel, Bus.Wait, ParkingCost, BusFare))
# data is wide so adding id col is not necessary
# 2) fit MNL model
com = mlogit.data(commute, choice = "Choice", shape = "wide",  varying = 3:6, sep= "_")
head(com)
PROBLEM 1
commute = read.csv('commuteFall2020.csv')
# 1) data preprocessing
# create columns for total travel time -- bus.wait + bus.travel; car.travel
commute$time_CAR = commute$Car.Travel
commute$time_BUS = commute$Bus.Travel + commute$Bus.Wait
# create columns for cost -- BusFare; ParkingCost
commute$cost_CAR = commute$ParkingCost
commute$cost_BUS = commute$BusFare
# drop unnecessary columns from df
commute = subset(commute, select = -c(Car.Travel, Bus.Travel, Bus.Wait, ParkingCost, BusFare))
# data is wide so adding id col is not necessary
View(commute)
View(commute)
